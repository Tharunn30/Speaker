{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1b0PJOCha4pP6cmhbmDOBRN8PyP8LWXoY","timestamp":1688993317754}],"gpuType":"T4","authorship_tag":"ABX9TyOIEE3necyjsGKvxfGOiCHI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fE2uIQtctXJ2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009080122,"user_tz":-330,"elapsed":19653,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"5e485dc6-2c24-438c-82aa-7fac230f60e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Dataset_voxlingua107/Train')\n","languages = sorted(os.listdir())\n","print('Current Directory :', os.getcwd(), '\\nDirectory Contents:', languages)\n","nLanguages = len(languages)"],"metadata":{"id":"6p3Ar4fKtbFO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009080123,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"d6d41bb7-498a-47eb-95d3-969943fa38a9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Directory : /content/drive/MyDrive/Dataset_voxlingua107/Train \n","Directory Contents: ['English', 'Hindi', 'Tamil', 'Telugu']\n"]}]},{"cell_type":"code","source":["!pip install speechbrain"],"metadata":{"id":"ymK2_mHptxIU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009090940,"user_tz":-330,"elapsed":10821,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"8a4fdc6c-0919-4008-d9ec-9187d508be11"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting speechbrain\n","  Downloading speechbrain-0.5.14-py3-none-any.whl (519 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m337.9/519.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperpyyaml (from speechbrain)\n","  Downloading HyperPyYAML-1.2.1-py3-none-any.whl (16 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.25.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.11.1)\n","Collecting sentencepiece (from speechbrain)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.1+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.2+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.65.0)\n","Collecting huggingface-hub (from speechbrain)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0)\n","Collecting ruamel.yaml<=0.17.28,>=0.17.8 (from hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml-0.17.28-py3-none-any.whl (109 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml<=0.17.28,>=0.17.8->hyperpyyaml->speechbrain)\n","  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n","Installing collected packages: sentencepiece, ruamel.yaml.clib, ruamel.yaml, huggingface-hub, hyperpyyaml, speechbrain\n","Successfully installed huggingface-hub-0.16.4 hyperpyyaml-1.2.1 ruamel.yaml-0.17.28 ruamel.yaml.clib-0.2.7 sentencepiece-0.1.99 speechbrain-0.5.14\n"]}]},{"cell_type":"code","source":["!pip install torchaudio"],"metadata":{"id":"lLrhtHCRwetx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009096350,"user_tz":-330,"elapsed":5415,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"6cb07b76-fb8f-4049-b37c-d1cc5496785a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchaudio) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchaudio) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchaudio) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchaudio) (1.3.0)\n"]}]},{"cell_type":"code","source":["\n","# import os\n","import torch  # noqa: F401\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from speechbrain.dataio.dataio import length_to_mask\n","from speechbrain.nnet.CNN import Conv1d as _Conv1d\n","from speechbrain.nnet.normalization import BatchNorm1d as _BatchNorm1d\n","from speechbrain.nnet.linear import Linear\n","\n","\n","# Skip transpose as much as possible for efficiency\n","class Conv1d(_Conv1d):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(skip_transpose=True, *args, **kwargs)\n","\n","\n","class BatchNorm1d(_BatchNorm1d):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(skip_transpose=True, *args, **kwargs)\n","\n","\n","class TDNNBlock(nn.Module):\n","    \"\"\"An implementation of TDNN.\n","\n","    Arguments\n","    ----------\n","    in_channels : int\n","        Number of input channels.\n","    out_channels : int\n","        The number of output channels.\n","    kernel_size : int\n","        The kernel size of the TDNN blocks.\n","    dilation : int\n","        The dilation of the Res2Net block.\n","    activation : torch class\n","        A class for constructing the activation layers.\n","\n","    Example\n","    -------\n","    >>> inp_tensor = torch.rand([8, 120, 64]).transpose(1, 2)\n","    >>> layer = TDNNBlock(64, 64, kernel_size=3, dilation=1)\n","    >>> out_tensor = layer(inp_tensor).transpose(1, 2)\n","    >>> out_tensor.shape\n","    torch.Size([8, 120, 64])\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel_size,\n","        dilation,\n","        activation=nn.ReLU,\n","    ):\n","        super(TDNNBlock, self).__init__()\n","        self.conv = Conv1d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            dilation=dilation,\n","        )\n","        self.activation = activation()\n","        self.norm = BatchNorm1d(input_size=out_channels)\n","\n","    def forward(self, x):\n","        return self.norm(self.activation(self.conv(x)))\n","\n","\n","class Res2NetBlock(torch.nn.Module):\n","    \"\"\"An implementation of Res2NetBlock w/ dilation.\n","\n","    Arguments\n","    ---------\n","    in_channels : int\n","        The number of channels expected in the input.\n","    out_channels : int\n","        The number of output channels.\n","    scale : int\n","        The scale of the Res2Net block.\n","    kernel_size: int\n","        The kernel size of the Res2Net block.\n","    dilation : int\n","        The dilation of the Res2Net block.\n","\n","    Example\n","    -------\n","    >>> inp_tensor = torch.rand([8, 120, 64]).transpose(1, 2)\n","    >>> layer = Res2NetBlock(64, 64, scale=4, dilation=3)\n","    >>> out_tensor = layer(inp_tensor).transpose(1, 2)\n","    >>> out_tensor.shape\n","    torch.Size([8, 120, 64])\n","    \"\"\"\n","\n","    def __init__(\n","        self, in_channels, out_channels, scale=8, kernel_size=3, dilation=1\n","    ):\n","        super(Res2NetBlock, self).__init__()\n","        assert in_channels % scale == 0\n","        assert out_channels % scale == 0\n","\n","        in_channel = in_channels // scale\n","        hidden_channel = out_channels // scale\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                TDNNBlock(\n","                    in_channel,\n","                    hidden_channel,\n","                    kernel_size=kernel_size,\n","                    dilation=dilation,\n","                )\n","                for i in range(scale - 1)\n","            ]\n","        )\n","        self.scale = scale\n","\n","    def forward(self, x):\n","        y = []\n","        for i, x_i in enumerate(torch.chunk(x, self.scale, dim=1)):\n","            if i == 0:\n","                y_i = x_i\n","            elif i == 1:\n","                y_i = self.blocks[i - 1](x_i)\n","            else:\n","                y_i = self.blocks[i - 1](x_i + y_i)\n","            y.append(y_i)\n","        y = torch.cat(y, dim=1)\n","        return y\n","\n","\n","class SEBlock(nn.Module):\n","    \"\"\"An implementation of squeeze-and-excitation block.\n","\n","    Arguments\n","    ---------\n","    in_channels : int\n","        The number of input channels.\n","    se_channels : int\n","        The number of output channels after squeeze.\n","    out_channels : int\n","        The number of output channels.\n","\n","    Example\n","    -------\n","    >>> inp_tensor = torch.rand([8, 120, 64]).transpose(1, 2)\n","    >>> se_layer = SEBlock(64, 16, 64)\n","    >>> lengths = torch.rand((8,))\n","    >>> out_tensor = se_layer(inp_tensor, lengths).transpose(1, 2)\n","    >>> out_tensor.shape\n","    torch.Size([8, 120, 64])\n","    \"\"\"\n","\n","    def __init__(self, in_channels, se_channels, out_channels):\n","        super(SEBlock, self).__init__()\n","\n","        self.conv1 = Conv1d(\n","            in_channels=in_channels, out_channels=se_channels, kernel_size=1\n","        )\n","        self.relu = torch.nn.ReLU(inplace=True)\n","        self.conv2 = Conv1d(\n","            in_channels=se_channels, out_channels=out_channels, kernel_size=1\n","        )\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","    def forward(self, x, lengths=None):\n","        L = x.shape[-1]\n","        if lengths is not None:\n","            mask = length_to_mask(lengths * L, max_len=L, device=x.device)\n","            mask = mask.unsqueeze(1)\n","            total = mask.sum(dim=2, keepdim=True)\n","            s = (x * mask).sum(dim=2, keepdim=True) / total\n","        else:\n","            s = x.mean(dim=2, keepdim=True)\n","\n","        s = self.relu(self.conv1(s))\n","        s = self.sigmoid(self.conv2(s))\n","\n","        return s * x\n","\n","\n","class AttentiveStatisticsPooling(nn.Module):\n","    \"\"\"This class implements an attentive statistic pooling layer for each channel.\n","    It returns the concatenated mean and std of the input tensor.\n","\n","    Arguments\n","    ---------\n","    channels: int\n","        The number of input channels.\n","    attention_channels: int\n","        The number of attention channels.\n","\n","    Example\n","    -------\n","    >>> inp_tensor = torch.rand([8, 120, 64]).transpose(1, 2)\n","    >>> asp_layer = AttentiveStatisticsPooling(64)\n","    >>> lengths = torch.rand((8,))\n","    >>> out_tensor = asp_layer(inp_tensor, lengths).transpose(1, 2)\n","    >>> out_tensor.shape\n","    torch.Size([8, 1, 128])\n","    \"\"\"\n","\n","    def __init__(self, channels, attention_channels=128, global_context=True):\n","        super().__init__()\n","\n","        self.eps = 1e-12\n","        self.global_context = global_context\n","        if global_context:\n","            self.tdnn = TDNNBlock(channels * 3, attention_channels, 1, 1)\n","        else:\n","            self.tdnn = TDNNBlock(channels, attention_channels, 1, 1)\n","        self.tanh = nn.Tanh()\n","        self.conv = Conv1d(\n","            in_channels=attention_channels, out_channels=channels, kernel_size=1\n","        )\n","\n","    def forward(self, x, lengths=None):\n","        \"\"\"Calculates mean and std for a batch (input tensor).\n","\n","        Arguments\n","        ---------\n","        x : torch.Tensor\n","            Tensor of shape [N, C, L].\n","        \"\"\"\n","        L = x.shape[-1]\n","\n","        def _compute_statistics(x, m, dim=2, eps=self.eps):\n","            mean = (m * x).sum(dim)\n","            std = torch.sqrt(\n","                (m * (x - mean.unsqueeze(dim)).pow(2)).sum(dim).clamp(eps)\n","            )\n","            return mean, std\n","\n","        if lengths is None:\n","            lengths = torch.ones(x.shape[0], device=x.device)\n","\n","        # Make binary mask of shape [N, 1, L]\n","        mask = length_to_mask(lengths * L, max_len=L, device=x.device)\n","        mask = mask.unsqueeze(1)\n","\n","        # Expand the temporal context of the pooling layer by allowing the\n","        # self-attention to look at global properties of the utterance.\n","        if self.global_context:\n","            # torch.std is unstable for backward computation\n","            # https://github.com/pytorch/pytorch/issues/4320\n","            total = mask.sum(dim=2, keepdim=True).float()\n","            mean, std = _compute_statistics(x, mask / total)\n","            mean = mean.unsqueeze(2).repeat(1, 1, L)\n","            std = std.unsqueeze(2).repeat(1, 1, L)\n","            attn = torch.cat([x, mean, std], dim=1)\n","        else:\n","            attn = x\n","\n","        # Apply layers\n","        attn = self.conv(self.tanh(self.tdnn(attn)))\n","\n","        # Filter out zero-paddings\n","        attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n","\n","        attn = F.softmax(attn, dim=2)\n","        mean, std = _compute_statistics(x, attn)\n","        # Append mean and std of the batch\n","        pooled_stats = torch.cat((mean, std), dim=1)\n","        pooled_stats = pooled_stats.unsqueeze(2)\n","\n","        return pooled_stats\n","\n","\n","class SERes2NetBlock(nn.Module):\n","    \"\"\"An implementation of building block in ECAPA-TDNN, i.e.,\n","    TDNN-Res2Net-TDNN-SEBlock.\n","\n","    Arguments\n","    ----------\n","    out_channels: int\n","        The number of output channels.\n","    res2net_scale: int\n","        The scale of the Res2Net block.\n","    kernel_size: int\n","        The kernel size of the TDNN blocks.\n","    dilation: int\n","        The dilation of the Res2Net block.\n","    activation : torch class\n","        A class for constructing the activation layers.\n","\n","    Example\n","    -------\n","    >>> x = torch.rand(8, 120, 64).transpose(1, 2)\n","    >>> conv = SERes2NetBlock(64, 64, res2net_scale=4)\n","    >>> out = conv(x).transpose(1, 2)\n","    >>> out.shape\n","    torch.Size([8, 120, 64])\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        res2net_scale=8,\n","        se_channels=128,\n","        kernel_size=1,\n","        dilation=1,\n","        activation=torch.nn.ReLU,\n","    ):\n","        super().__init__()\n","        self.out_channels = out_channels\n","        self.tdnn1 = TDNNBlock(\n","            in_channels,\n","            out_channels,\n","            kernel_size=1,\n","            dilation=1,\n","            activation=activation,\n","        )\n","        self.res2net_block = Res2NetBlock(\n","            out_channels, out_channels, res2net_scale, kernel_size, dilation\n","        )\n","        self.tdnn2 = TDNNBlock(\n","            out_channels,\n","            out_channels,\n","            kernel_size=1,\n","            dilation=1,\n","            activation=activation,\n","        )\n","        self.se_block = SEBlock(out_channels, se_channels, out_channels)\n","\n","        self.shortcut = None\n","        if in_channels != out_channels:\n","            self.shortcut = Conv1d(\n","                in_channels=in_channels,\n","                out_channels=out_channels,\n","                kernel_size=1,\n","            )\n","\n","    def forward(self, x, lengths=None):\n","        residual = x\n","        if self.shortcut:\n","            residual = self.shortcut(x)\n","\n","        x = self.tdnn1(x)\n","        x = self.res2net_block(x)\n","        x = self.tdnn2(x)\n","        x = self.se_block(x, lengths)\n","\n","        return x + residual\n","\n","\n","class ECAPA_TDNN(torch.nn.Module):\n","    \"\"\"An implementation of the speaker embedding model in a paper.\n","    \"ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in\n","    TDNN Based Speaker Verification\" (https://arxiv.org/abs/2005.07143).\n","\n","    Arguments\n","    ---------\n","    device : str\n","        Device used, e.g., \"cpu\" or \"cuda\".\n","    activation : torch class\n","        A class for constructing the activation layers.\n","    channels : list of ints\n","        Output channels for TDNN/SERes2Net layer.\n","    kernel_sizes : list of ints\n","        List of kernel sizes for each layer.\n","    dilations : list of ints\n","        List of dilations for kernels in each layer.\n","    lin_neurons : int\n","        Number of neurons in linear layers.\n","\n","    Example\n","    -------\n","    >>> input_feats = torch.rand([5, 120, 80])\n","    >>> compute_embedding = ECAPA_TDNN(80, lin_neurons=192)\n","    >>> outputs = compute_embedding(input_feats)\n","    >>> outputs.shape\n","    torch.Size([5, 1, 192])\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        input_size,\n","        device=\"cpu\",\n","        lin_neurons=192,\n","        activation=torch.nn.ReLU,\n","        channels=[512, 512, 512, 512, 1536],\n","        kernel_sizes=[5, 3, 3, 3, 1],\n","        dilations=[1, 2, 3, 4, 1],\n","        attention_channels=128,\n","        res2net_scale=8,\n","        se_channels=128,\n","        global_context=True,\n","    ):\n","\n","        super().__init__()\n","        assert len(channels) == len(kernel_sizes)\n","        assert len(channels) == len(dilations)\n","        self.channels = channels\n","        self.blocks = nn.ModuleList()\n","\n","        # The initial TDNN layer\n","        self.blocks.append(\n","            TDNNBlock(\n","                input_size,\n","                channels[0],\n","                kernel_sizes[0],\n","                dilations[0],\n","                activation,\n","            )\n","        )\n","\n","        # SE-Res2Net layers\n","        for i in range(1, len(channels) - 1):\n","            self.blocks.append(\n","                SERes2NetBlock(\n","                    channels[i - 1],\n","                    channels[i],\n","                    res2net_scale=res2net_scale,\n","                    se_channels=se_channels,\n","                    kernel_size=kernel_sizes[i],\n","                    dilation=dilations[i],\n","                    activation=activation,\n","                )\n","            )\n","\n","        # Multi-layer feature aggregation\n","        self.mfa = TDNNBlock(\n","            channels[-1],\n","            channels[-1],\n","            kernel_sizes[-1],\n","            dilations[-1],\n","            activation,\n","        )\n","\n","        # Attentive Statistical Pooling\n","        self.asp = AttentiveStatisticsPooling(\n","            channels[-1],\n","            attention_channels=attention_channels,\n","            global_context=global_context,\n","        )\n","        self.asp_bn = BatchNorm1d(input_size=channels[-1] * 2)\n","\n","        # Final linear transformation\n","        self.fc = Conv1d(\n","            in_channels=channels[-1] * 2,\n","            out_channels=lin_neurons,\n","            kernel_size=1,\n","        )\n","\n","    def forward(self, x, lengths=None):\n","        \"\"\"Returns the embedding vector.\n","\n","        Arguments\n","        ---------\n","        x : torch.Tensor\n","            Tensor of shape (batch, time, channel).\n","        \"\"\"\n","          # Minimize transpose for efficiency\n","        if x.dim() >= 3:\n","          x = x.transpose(1, 2)\n","\n","        xl = []\n","        for layer in self.blocks:\n","            try:\n","                x = layer(x, lengths=lengths)\n","            except TypeError:\n","                x = layer(x)\n","            xl.append(x)\n","\n","        # Multi-layer feature aggregation\n","        x = torch.cat(xl[1:], dim=1)\n","        x = self.mfa(x)\n","\n","        # Attentive Statistical Pooling\n","        x = self.asp(x, lengths=lengths)\n","        x = self.asp_bn(x)\n","\n","        # Final linear transformation\n","        x = self.fc(x)\n","\n","        x = x.transpose(1, 2)\n","        return x\n","\n","\n","class Classifier(torch.nn.Module):\n","    \"\"\"This class implements the cosine similarity on the top of features.\n","\n","    Arguments\n","    ---------\n","    device : str\n","        Device used, e.g., \"cpu\" or \"cuda\".\n","    lin_blocks : int\n","        Number of linear layers.\n","    lin_neurons : int\n","        Number of neurons in linear layers.\n","    out_neurons : int\n","        Number of classes.\n","\n","    Example\n","    -------\n","    >>> classify = Classifier(input_size=2, lin_neurons=2, out_neurons=2)\n","    >>> outputs = torch.tensor([ [1., -1.], [-9., 1.], [0.9, 0.1], [0.1, 0.9] ])\n","    >>> outupts = outputs.unsqueeze(1)\n","    >>> cos = classify(outputs)\n","    >>> (cos < -1.0).long().sum()\n","    tensor(0)\n","    >>> (cos > 1.0).long().sum()\n","    tensor(0)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        input_size,\n","        device=\"cpu\",\n","        lin_blocks=0,\n","        lin_neurons=192,\n","        out_neurons=1211,\n","    ):\n","\n","        super().__init__()\n","        self.blocks = nn.ModuleList()\n","\n","        for block_index in range(lin_blocks):\n","            self.blocks.extend(\n","                [\n","                    _BatchNorm1d(input_size),\n","                    Linear(input_size=input_size, n_neurons=lin_neurons),\n","                ]\n","            )\n","            input_size = lin_neurons\n","\n","        # Final Layer\n","        self.weight = nn.Parameter(\n","            torch.FloatTensor(out_neurons, input_size, device=device)\n","        )\n","        nn.init.xavier_uniform_(self.weight)\n","\n","    def forward(self, x):\n","        \"\"\"Returns the output probabilities over speakers.\n","\n","        Arguments\n","        ---------\n","        x : torch.Tensor\n","            Torch tensor.\n","        \"\"\"\n","        for layer in self.blocks:\n","            x = layer(x)\n","\n","        # Need to be normalized\n","        x = F.linear(F.normalize(x.squeeze(1)), F.normalize(self.weight))\n","        return x.unsqueeze(1)"],"metadata":{"id":"EsKpIbk0tm9i","executionInfo":{"status":"ok","timestamp":1689009104918,"user_tz":-330,"elapsed":8579,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torchaudio\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn import preprocessing\n","\n","# Define the directory paths of your train and test data\n","train_dir = '/content/drive/MyDrive/Dataset_voxlingua107/Train'\n","test_dir = '/content/drive/MyDrive/Dataset_voxlingua107/Test'\n","\n","# Define the list of languages\n","languages = ['Tamil']\n","\n","# Load and process the train data\n","train_data = []\n","max_waveform_length = 0\n","file_count = 0  # Counter variable to track the number of selected files\n","\n","for language in languages:\n","    language_dir = os.path.join(train_dir, language)\n","    for filename in os.listdir(language_dir):\n","        if file_count >= 100:\n","            break  # Break out of the loop if 1000 files are already selected\n","\n","        file_path = os.path.join(language_dir, filename)\n","        waveform, sample_rate = torchaudio.load(file_path)\n","\n","        train_data.append((waveform,language))\n","\n","        file_count += 1  # Increment the counter variable\n","        waveform_length = waveform.shape[1]\n","        if waveform_length > max_waveform_length:\n","            max_waveform_length = waveform_length\n","\n","# for language in languages:\n","#     language_dir = os.path.join(train_dir, language)\n","#     for filename in os.listdir(language_dir):\n","#         file_path = os.path.join(language_dir, filename)\n","#         waveform, sample_rate = torchaudio.load(file_path)\n","\n","#         # Perform any necessary preprocessing on the waveform\n","\n","#         train_data.append((waveform, language))\n","\n","#         waveform_length = waveform.shape[1]\n","#         if waveform_length > max_waveform_length:\n","#             max_waveform_length = waveform_length\n","\n","# Load and process the test data\n","test_data = []\n","for language in languages:\n","  language_dir = os.path.join(test_dir, language)\n","  for filename in os.listdir(language_dir):\n","         file_path = os.path.join(language_dir, filename)\n","         waveform, sample_rate = torchaudio.load(file_path)\n","\n","#         # Perform any necessary preprocessing on the waveform\n","\n","         test_data.append((waveform, language))\n","\n","# Convert the list of waveforms and labels to tensors\n","train_waveforms, train_labels = zip(*train_data)\n","test_waveforms, test_labels = zip(*test_data)\n","#train_data['train_labels'] = train_data['train_labels'].replace(['Tamil'],[0])\n","\n","le = preprocessing.LabelEncoder()\n","train_labels = le.fit_transform(train_labels)\n","# targets: array([0, 1, 2, 3, 4])\n","\n","train_labels = torch.as_tensor(train_labels)\n","#train_labels = torch.tensor(train_labels)\n","#train_labels = train_labels[:129440]\n","train_waveforms = pad_sequence(train_waveforms, batch_first=True)\n","test_labels = torch.tensor(test_labels)\n","test_waveforms = pad_sequence(test_waveforms, batch_first=True)\n","print(test_labels.shape)\n","\n","# Print the shapes of the train and test data tensors\n","print(\"Train data shape:\", train_waveforms.shape)\n","print(\"Train labels shape:\", train_labels.shape)\n","print(\"Test data shape:\", test_waveforms.shape)\n","print(\"Test labels shape:\", test_labels.shape)\n","\n","print(train_waveforms)\n","\n"],"metadata":{"id":"IlkN2H8WwXbj","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"error","timestamp":1689009224465,"user_tz":-330,"elapsed":119571,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"5a547647-1cb5-411c-c3b7-86e9209d6cb7"},"execution_count":6,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-5cfd71a2906e>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m#train_labels = torch.tensor(train_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m#train_labels = train_labels[:129440]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mtrain_waveforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_waveforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mtest_waveforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_waveforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (129440) must match the size of tensor b (273280) at non-singleton dimension 1"]}]},{"cell_type":"code","source":["train_waveforms[0].shape"],"metadata":{"id":"F4rCMZLPDSup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009383516,"user_tz":-330,"elapsed":596,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"5fb2fa6f-7212-4f23-df5c-adb4c869a29f"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 129440])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_waveforms[1].shape"],"metadata":{"id":"ebAFP9-7LRad","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689009395739,"user_tz":-330,"elapsed":637,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"3c3f6755-aa75-4760-d872-d6bf939134b5"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 273280])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["waveform_shape = []\n","for tensor in train_waveforms:\n","  tensor_shape = tensor.shape\n","  waveform_shape.append(tensor_shape)\n","\n","waveform_shape"],"metadata":{"id":"yKjOMpWpLV_L","executionInfo":{"status":"aborted","timestamp":1689009224468,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# computing the embeddings on train waveforms using ECAPA-TDNN\n","compute_embedding = ECAPA_TDNN(80, lin_neurons=192)\n","for embed in train_waveforms:\n","  outputs = compute_embedding(embed)\n","\n"],"metadata":{"id":"PwAEzGj-trza","executionInfo":{"status":"aborted","timestamp":1689009224468,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","label_plot = zip(tam_index,tam_textlab)\n","labelDf = pd.DataFrame(label_plot)\n","labelDf.head()"],"metadata":{"id":"hE8G5gD4wI5n","executionInfo":{"status":"aborted","timestamp":1689009224468,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import pandas as pd\n","import torch\n","\n","\n","new_data = []\n","plotLabels = []\n","# Iterate over the rows of the DataFrame\n","for index, row in labelDf.iterrows():\n","    tensor_value = row[0][0]\n","    new_data.append(tensor_value)\n","    plotValue = row[1][0]\n","    plotLabels.append(plotValue)\n","\n","\n","PlotDf = pd.DataFrame(new_data)\n","plotLabelDf = pd.DataFrame(plotLabels)\n","\n","print(plotLabelDf)\n","\n","PlotDf = pd.concat([PlotDf, plotLabelDf], axis = 1)\n","PlotDf.columns = ['tensor_values','tensor_labels']\n","PlotDf\n"],"metadata":{"id":"GjnvQerByqcA","executionInfo":{"status":"aborted","timestamp":1689009224469,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cO8W7hf_5Zus","executionInfo":{"status":"aborted","timestamp":1689009224469,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"execution_count":null,"outputs":[]}]}