{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1UwisnAjr8nQF3UnrkIJ4abBMAWzVwBMh","timestamp":1693472848832},{"file_id":"1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz","timestamp":1618676214032}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uo0JP7a5uFp7"},"source":["# **Speech Classification From Scratch**\n","\n","Do you want to figure out how to implement a **classification** system with SpeechBrain? Look no further, you're in the right place. This tutorial will walk you through all the steps needed to implement an **utterance-level classifier** in SpeechBrain.  \n","The tutorial will initially focus on **speaker identification** and will describe, along the way, how to extend it to many other classification tasks such as **language-id**, **emotion recognition**, **sound classification**, **keyword spotting**, and many others.\n","\n","\n","## **Models**\n","Many neural models can be used to approach this kind of task. In this tutorial, we will focus on a **TDNN** classifier (*xvector*) and a very recent model called [**ECAPA-TDNN**](https://arxiv.org/abs/2005.07143) that showed impressive performance in speaker verification and diarization.\n","\n","## **Data**\n","Training will be done with a small open-source dataset called [mini-librispeech](https://www.openslr.org/31/), which only contains few hours of training data. In a real case, you need a much larger dataset.\n","For some examples on a real task, please [take a look into our Voxceleb recipes](https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb/SpeakerRec).\n","\n","## **Code**\n","In this tutorial, we will refer to the code in [```speechbrain/templates/speaker_id```](https://github.com/speechbrain/speechbrain/tree/develop/templates/speaker_id).\n","\n","## **Installation**\n","Before starting, let's install speechbrain:"]},{"cell_type":"code","metadata":{"id":"beagAGw5t5bK","executionInfo":{"status":"ok","timestamp":1693464462443,"user_tz":-330,"elapsed":50195,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["%%capture\n","# Local installation\n","!git clone https://github.com/speechbrain/speechbrain/\n","%cd /content/speechbrain/\n","!pip install -r requirements.txt\n","!pip install -e ."],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2xRRTu12y1w","executionInfo":{"status":"ok","timestamp":1693464469690,"user_tz":-330,"elapsed":7251,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["%%capture\n","# For pip installation\n","!pip install speechbrain"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eWpl9xgAIXKE"},"source":["## **Which steps are needed?**\n","Training an utterance-level classifier is relatively easy in SpeechBrain. The steps to follows are:\n","\n","1. **Prepare your data**.\n","The goal of this step is to create the data manifest files (in CSV or JSON format). The data manifest files tell SpeechBrain where to find the speech data and their corresponding utterance-level classification (e.g., speaker id). In this tutorial, the data manifest files are created by [mini_librispeech_prepare.py](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/mini_librispeech_prepare.py).  \n","\n","2. **Train the classifier**.\n","At this point, we are ready to train our classifier.\n","To train  a speaker-id classifier based on TDNN + statistical pooling (xvectors), run the following command:\n","```\n","cd speechbrain/templates/speaker_id/\n","python train.py train.yaml\n","```\n","Later, we will describe how to plug another model called Emphasized Channel Attention, Propagation, and Aggregation model (ECAPA) that turned out to provide impressive performance in speaker recognition tasks.\n","\n","3. **Use the classifier (inference)**:\n","After training, we can use the classifier for inference. A class called `EncoderClassifier` is designed to make inference easier. We also designed a class called `SpeakerRecognition` to make inference on a speaker verification task easier.\n","\n","\n","\n","We will now provide a detailed description of all these steps."]},{"cell_type":"markdown","metadata":{"id":"rDgNu_b8k6qD"},"source":["## **Step 1: Prepare your data**\n","The goal of data preparation is to create the data manifest files.\n","These files tell SpeechBrain where to find the audio data and their corresponding utterance-level classification. They are text files written in the popular CSV and JSON formats.\n","\n","### **Data manifest files**\n","Let's take a look into how a data manifest file in JSON format looks like:\n","\n","\n","```json\n","{\n","  \"163-122947-0045\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/163/122947/163-122947-0045.flac\",\n","    \"length\": 14.335,\n","    \"spk_id\": \"163\"\n","  },\n","  \"7312-92432-0025\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7312/92432/7312-92432-0025.flac\",\n","    \"length\": 12.01,\n","    \"spk_id\": \"7312\"\n","  },\n","  \"7859-102519-0036\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7859/102519/7859-102519-0036.flac\",\n","    \"length\": 11.965,\n","    \"spk_id\": \"7859\"\n","  },\n","}\n","```\n","As you can see, we have a hierarchical structure in which the first key is a **unique identifier** of the spoken sentence.\n","Then, we specify all the fields that are needed for the task addressed. For instance, we report the **path of the speech recording**, its **length** in seconds (needed if we wanna sort the sentences before creating the mini-batches), and the **speaker identity** of the speaker in the given recording.\n","\n","\n","Actually, you can specify here all entries that you need (language-id, emotion annotation, etc). However, there must be a matching between the name of these entries and what the experiment script (e.g, train.py) expects. We will elaborate more on this later.\n","\n","You might have noticed that we define a special variable called `data_root`. This allows users to dynamically change the data folder from the command line (or from the yaml hyperparameter file).\n","\n","\n","### **Preparation Script**\n","**Every dataset is formatted in a different way**. The script that parses your own dataset and creates the JSON or the CSV files is something that you are supposed to write. Most of the time, this is very straightforward.\n","\n","For the mini-librispeech dataset, for instance, we wrote this simple data preparation script called [mini_librispeech_prepare.py](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/mini_librispeech_prepare.py).\n","\n","This function automatically downloads the data (that in this case are publicly available). We search for all the audio files and while reading them we create the JSON file with the speaker-id annotation.\n","\n","You can use this script as a good base for your **custom preparation** on your target dataset. As you can see, we create three separate data manifest files to manage training, validation, and test phases.\n","\n","\n","### **Copy your data locally**\n","When using speechbrain (or any other toolkit) within an HPC cluster, a good practice is to compress your dataset in a single file and **copy (and uncompress) the data in the local folder of the computing node**. This would make the code much **much faster** because the data aren't fetched from the shared filesystem but from the local one. Moreover, you don't harm the performance of the shared filesystem with tons of reading operations. We **strongly suggest users follow this approach** (not possible here in Google Colab)."]},{"cell_type":"markdown","metadata":{"id":"nkYENC7BJ4K9"},"source":["## **Step 2: Train the classifier**\n","We show now how we can train an **utterance-level classifier** with SpeechBrain.\n","The proposed recipe performs a feature computation/normalization, processes the features with an encoder, and applies a classifier on top of that. Data augmentation is also employed to improve system performance.\n","\n","### **Train a speaker-id model**\n","\n","We are going to train the TDNN-based model used for x-vectors. Statistical pooling is used on the top of the convolutional layers to convert a variable-length sentence into a **fixed-length embeddings**.\n","\n","On the top of the embeddings, a simple fully-connected classifier is employed to predict which of the N speakers is active in the given sentence.\n","\n","\n","To train this model, run the following code:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AtMw7x0ybFlI","outputId":"4c7e41b0-15fa-4a32-8761-3b421cae3de9","executionInfo":{"status":"ok","timestamp":1693465700001,"user_tz":-330,"elapsed":1230314,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["%cd /content/speechbrain/templates/speaker_id\n","!python train.py train.yaml --number_of_epochs=15 #--device='cpu'"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/speechbrain/templates/speaker_id\n","Downloading http://www.openslr.org/resources/28/rirs_noises.zip to ./data/rirs_noises.zip\n","rirs_noises.zip: 1.31GB [00:48, 26.8MB/s]                \n","Extracting ./data/rirs_noises.zip to ./data\n","speechbrain.core - Beginning experiment!\n","speechbrain.core - Experiment folder: ./results/speaker_id/1986\n","Downloading http://www.openslr.org/resources/31/train-clean-5.tar.gz to ./data/train-clean-5.tar.gz\n","train-clean-5.tar.gz: 333MB [00:13, 23.9MB/s]               \n","mini_librispeech_prepare - Creating train.json, valid.json, and test.json\n","mini_librispeech_prepare - train.json successfully created!\n","mini_librispeech_prepare - valid.json successfully created!\n","mini_librispeech_prepare - test.json successfully created!\n","speechbrain.dataio.encoder - Load called, but CategoricalEncoder is not empty. Loaded data will overwrite everything. This is normal if there is e.g. an unk label defined at init.\n","speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n","speechbrain.core - 4.5M trainable parameters in SpkIdBrain\n","speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n","speechbrain.utils.epoch_loop - Going into epoch 1\n","  0% 0/76 [00:00<?, ?it/s]/content/speechbrain/speechbrain/dataio/encoder.py:722: UserWarning: CategoricalEncoder.expect_len was never called: assuming category count of 28 to be correct! Sanity check your encoder using `.expect_len`. Ensure that downstream code also uses the correct size. If you are sure this does not apply to you, use `.ignore_len`.\n","  warnings.warn(\n","100% 76/76 [01:22<00:00,  1.09s/it, train_loss=1.8]\n","100% 10/10 [00:01<00:00,  5.77it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.001 to 0.00094\n","speechbrain.utils.train_logger - Epoch: 1, lr: 1.00e-03 - train loss: 1.80 - valid loss: 1.66, valid error: 5.56e-01\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-51-19+00\n","speechbrain.utils.epoch_loop - Going into epoch 2\n","100% 76/76 [01:10<00:00,  1.07it/s, train_loss=0.834]\n","100% 10/10 [00:01<00:00,  6.29it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00094 to 0.00087\n","speechbrain.utils.train_logger - Epoch: 2, lr: 9.36e-04 - train loss: 8.34e-01 - valid loss: 3.17e-01, valid error: 9.27e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-52-32+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-51-19+00\n","speechbrain.utils.epoch_loop - Going into epoch 3\n","100% 76/76 [01:11<00:00,  1.06it/s, train_loss=0.633]\n","100% 10/10 [00:01<00:00,  8.42it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00087 to 0.00081\n","speechbrain.utils.train_logger - Epoch: 3, lr: 8.71e-04 - train loss: 6.33e-01 - valid loss: 1.96e-01, valid error: 4.64e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-53-44+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-52-32+00\n","speechbrain.utils.epoch_loop - Going into epoch 4\n","100% 76/76 [01:09<00:00,  1.09it/s, train_loss=0.446]\n","100% 10/10 [00:01<00:00,  6.39it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00081 to 0.00074\n","speechbrain.utils.train_logger - Epoch: 4, lr: 8.07e-04 - train loss: 4.46e-01 - valid loss: 1.65e-01, valid error: 5.96e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-54-56+00\n","speechbrain.utils.epoch_loop - Going into epoch 5\n","100% 76/76 [01:10<00:00,  1.07it/s, train_loss=0.364]\n","100% 10/10 [00:01<00:00,  8.29it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00074 to 0.00068\n","speechbrain.utils.train_logger - Epoch: 5, lr: 7.43e-04 - train loss: 3.64e-01 - valid loss: 1.04, valid error: 3.31e-01\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-56-08+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-54-56+00\n","speechbrain.utils.epoch_loop - Going into epoch 6\n","100% 76/76 [01:11<00:00,  1.07it/s, train_loss=0.367]\n","100% 10/10 [00:01<00:00,  8.38it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00068 to 0.00061\n","speechbrain.utils.train_logger - Epoch: 6, lr: 6.79e-04 - train loss: 3.67e-01 - valid loss: 5.87e-02, valid error: 1.32e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-57-21+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-56-08+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-53-44+00\n","speechbrain.utils.epoch_loop - Going into epoch 7\n","100% 76/76 [01:10<00:00,  1.08it/s, train_loss=0.237]\n","100% 10/10 [00:01<00:00,  8.11it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00061 to 0.00055\n","speechbrain.utils.train_logger - Epoch: 7, lr: 6.14e-04 - train loss: 2.37e-01 - valid loss: 3.69e-01, valid error: 1.46e-01\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-58-32+00\n","speechbrain.utils.epoch_loop - Going into epoch 8\n","100% 76/76 [01:11<00:00,  1.06it/s, train_loss=0.253]\n","100% 10/10 [00:01<00:00,  7.93it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00055 to 0.00049\n","speechbrain.utils.train_logger - Epoch: 8, lr: 5.50e-04 - train loss: 2.53e-01 - valid loss: 1.45e-01, valid error: 5.96e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-59-45+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-58-32+00\n","speechbrain.utils.epoch_loop - Going into epoch 9\n","100% 76/76 [01:10<00:00,  1.08it/s, train_loss=0.21]\n","100% 10/10 [00:01<00:00,  8.07it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00049 to 0.00042\n","speechbrain.utils.train_logger - Epoch: 9, lr: 4.86e-04 - train loss: 2.10e-01 - valid loss: 6.97e-02, valid error: 1.32e-02\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-00-57+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-57-21+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+06-59-45+00\n","speechbrain.utils.epoch_loop - Going into epoch 10\n","100% 76/76 [01:11<00:00,  1.06it/s, train_loss=0.179]\n","100% 10/10 [00:01<00:00,  8.33it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00042 to 0.00036\n","speechbrain.utils.train_logger - Epoch: 10, lr: 4.21e-04 - train loss: 1.79e-01 - valid loss: 2.35e-02, valid error: 0.00e+00\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-02-09+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-00-57+00\n","speechbrain.utils.epoch_loop - Going into epoch 11\n","100% 76/76 [01:10<00:00,  1.08it/s, train_loss=0.155]\n","100% 10/10 [00:01<00:00,  6.12it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00036 to 0.00029\n","speechbrain.utils.train_logger - Epoch: 11, lr: 3.57e-04 - train loss: 1.55e-01 - valid loss: 8.89e-03, valid error: 0.00e+00\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-03-22+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-02-09+00\n","speechbrain.utils.epoch_loop - Going into epoch 12\n","100% 76/76 [01:11<00:00,  1.06it/s, train_loss=0.123]\n","100% 10/10 [00:01<00:00,  6.01it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00029 to 0.00023\n","speechbrain.utils.train_logger - Epoch: 12, lr: 2.93e-04 - train loss: 1.23e-01 - valid loss: 1.21e-02, valid error: 6.62e-03\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-04-35+00\n","speechbrain.utils.epoch_loop - Going into epoch 13\n","100% 76/76 [01:10<00:00,  1.07it/s, train_loss=0.111]\n","100% 10/10 [00:01<00:00,  5.89it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00023 to 0.00016\n","speechbrain.utils.train_logger - Epoch: 13, lr: 2.29e-04 - train loss: 1.11e-01 - valid loss: 9.37e-03, valid error: 0.00e+00\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-05-48+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-03-22+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-04-35+00\n","speechbrain.utils.epoch_loop - Going into epoch 14\n","100% 76/76 [01:11<00:00,  1.07it/s, train_loss=0.0931]\n","100% 10/10 [00:01<00:00,  8.28it/s]\n","speechbrain.nnet.schedulers - Changing lr from 0.00016 to 0.0001\n","speechbrain.utils.train_logger - Epoch: 14, lr: 1.64e-04 - train loss: 9.31e-02 - valid loss: 3.55e-03, valid error: 0.00e+00\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-07-00+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-05-48+00\n","speechbrain.utils.epoch_loop - Going into epoch 15\n","100% 76/76 [01:11<00:00,  1.07it/s, train_loss=0.0741]\n","100% 10/10 [00:01<00:00,  8.23it/s]\n","speechbrain.utils.train_logger - Epoch: 15, lr: 1.00e-04 - train loss: 7.41e-02 - valid loss: 3.98e-03, valid error: 0.00e+00\n","speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-08-13+00\n","speechbrain.utils.checkpoints - Deleted checkpoint in results/speaker_id/1986/save/CKPT+2023-08-31+07-07-00+00\n","speechbrain.utils.checkpoints - Loading a checkpoint from results/speaker_id/1986/save/CKPT+2023-08-31+07-08-13+00\n","100% 10/10 [00:01<00:00,  6.87it/s]\n","speechbrain.utils.train_logger - Epoch loaded: 15 - test loss: 5.64e-03, test error: 0.00e+00\n"]}]},{"cell_type":"markdown","metadata":{"id":"b3tnXnrWc2My"},"source":["As you can see from the prints, both the validation and training **losses are decreasing** very fast in the first epochs. Then, we basically see some minor improvements and performance oscillations.\n","\n","At the end of the training, the **validation error should go to zero** (or very close to zero).\n","\n","The task proposed in this tutorial is very easy because we only have to classify the 28 speakers of the mini-librispeech dataset. Take this tutorial just as an example that explains how to set up all the needed components to develop a speech classifier. [Please, refer to our voxceleb recipes if you would like to see an example on a popular speaker recognition dataset](https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb)\n","\n","\n","\n","Before diving into the code, let's see which files/folders are generated in the specified `output_folder`:\n","\n","*   `train_log.txt`: contains the statistics (e.g, train_loss, valid_loss) computed at each epoch.\n","*   `log.txt`: is a more detailed logger containing the timestamps for each basic operation.\n","*  `env.log`: shows all the dependencies used with their corresponding version (useful for replicability).\n","\n","*  `train.py`, `hyperparams.yaml`:  are a copy of the experiment file along with the corresponding hyperparameters (for replicability).\n","\n","* `save`:  is the place where we store the learned model.\n","\n","In the `save` folder, you find subfolders containing the checkpoints saved during training (in the format `CKPT+data+time`). Typically, you find here two checkpoints: the **best** (i.e, the oldest one) and the **latest** (i.e, the most recent one). If you find only a single checkpoint it means that the last epoch is also the best.\n","\n","Inside each checkpoint, we store all the information needed to **resume training** (e.g, models, optimizers, schedulers, epoch counter, etc.). The parameters of the embedding models are reported in `embedding_model.ckpt` file,\n","while the ones of the classifier are in `classifier.ckpt`. This is just a binary format readable with `torch.load`.\n","\n","The save folder contains the **label encoder** (`label_encoder.txt`) as well, which maps each speaker-id entry to their corresponding indexes.\n","\n","```\n","'163' => 0\n","'7312' => 1\n","'7859' => 2\n","'19' => 3\n","'1737' => 4\n","'6272' => 5\n","'1970' => 6\n","'2416' => 7\n","'118' => 8\n","'6848' => 9\n","'4680' => 10\n","'460' => 11\n","'3664' => 12\n","'3242' => 13\n","'1898' => 14\n","'7367' => 15\n","'1088' => 16\n","'3947' => 17\n","'3526' => 18\n","'1867' => 19\n","'8629' => 20\n","'332' => 21\n","'4640' => 22\n","'2136' => 23\n","'669' => 24\n","'5789' => 25\n","'32' => 26\n","'226' => 27\n","================\n","'starting_index' => 0\n","```\n","\n","\n","As usual, we implement the system with an experiment file `train.py` and a hyperparameter file called `train.yaml`.\n","\n","### **Hyperparameters**\n","The yaml file contains all the modules and hyperparameters need to implement the desired classifier.\n","[You can take a look into the full train.yaml file here](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/train.yaml).\n","\n","In the first part, we specify some basic settings, such as the seed and the path of the output folder:\n","\n","```yaml\n","# Seed needs to be set at top of yaml, before objects with parameters are made\n","seed: 1986\n","__set_seed: !!python/object/apply:torch.manual_seed [!ref <seed>]\n","\n","# If you plan to train a system on an HPC cluster with a big dataset,\n","# we strongly suggest doing the following:\n","# 1- Compress the dataset in a single tar or zip file.\n","# 2- Copy your dataset locally (i.e., the local disk of the computing node).\n","# 3- Uncompress the dataset in the local folder.\n","# 4- Set data_folder with the local path.\n","# Reading data from the local disk of the compute node (e.g. $SLURM_TMPDIR with SLURM-based clusters) is very important.\n","# It allows you to read the data much faster without slowing down the shared filesystem.\n","data_folder: ./data\n","output_folder: !ref ./results/speaker_id/<seed>\n","save_folder: !ref <output_folder>/save\n","train_log: !ref <output_folder>/train_log.txt\n","```\n","\n","We then specify the path of the data manifest files for training, validation, and test:\n","\n","```yaml\n","# Path where data manifest files will be stored\n","# The data manifest files are created by the data preparation script.\n","train_annotation: train.json\n","valid_annotation: valid.json\n","test_annotation: test.json\n","```\n","\n","These files will be automatically created when calling the data preparation script ([mini_librispeech_prepare.py](https://github.com/speechbrain/speechbrain/blob/develop/templates/speaker_id/mini_librispeech_prepare.py)) from the experiment file (`train.py`).\n","\n","\n","Next, we set up the `train_logger` and declare the `error_stats` objects that will gather statistics on the classification error rate:\n","\n","\n","```yaml\n","# The train logger writes training statistics to a file, as well as stdout.\n","train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n","    save_file: !ref <train_log>\n","\n","error_stats: !name:speechbrain.utils.metric_stats.MetricStats\n","    metric: !name:speechbrain.nnet.losses.classification_error\n","        reduction: batch\n","```\n","\n","\n","We can now specify some training hyperparameters such as the number of epochs, the batch size, the learning rate, the number of epochs, and the embedding dimensionality.\n","\n","\n","```yaml\n","ckpt_interval_minutes: 15 # save checkpoint every N min\n","\n","# Feature parameters\n","n_mels: 23\n","\n","# Training Parameters\n","sample_rate: 16000\n","number_of_epochs: 35\n","batch_size: 16\n","lr_start: 0.001\n","lr_final: 0.0001\n","n_classes: 28 # In this case, we have 28 speakers\n","emb_dim: 512 # dimensionality of the embeddings\n","dataloader_options:\n","    batch_size: !ref <batch_size>\n","```\n","\n","The variable `ckpt_interval_minutes` can be used to save checkpoints every N minutes within a training epoch. In some cases, one epoch might take several hours, and saving the checkpoint periodically is a good and safe practice. This feature is not really needed for this simple tutorial based on a tiny dataset.\n","\n","We can now define the most important modules that are needed to train our model:\n","\n","```yaml\n","# Added noise and reverb come from OpenRIR dataset, automatically\n","# downloaded and prepared with this Environmental Corruption class.\n","env_corrupt: !new:speechbrain.lobes.augment.EnvCorrupt\n","    openrir_folder: !ref <data_folder>\n","    babble_prob: 0.0\n","    reverb_prob: 0.0\n","    noise_prob: 1.0\n","    noise_snr_low: 0\n","    noise_snr_high: 15\n","\n","# Adds speech change + time and frequency dropouts (time-domain implementation)\n","# # A small speed change help to improve the performance of speaker-id as well.\n","augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment\n","    sample_rate: !ref <sample_rate>\n","    speeds: [95, 100, 105]\n","\n","# Feature extraction\n","compute_features: !new:speechbrain.lobes.features.Fbank\n","    n_mels: !ref <n_mels>\n","\n","# Mean and std normalization of the input features\n","mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n","    norm_type: sentence\n","    std_norm: False\n","\n","# To design a custom model, either just edit the simple CustomModel\n","# class that's listed here, or replace this `!new` call with a line\n","# pointing to a different file you've defined.\n","embedding_model: !new:custom_model.Xvector\n","    in_channels: !ref <n_mels>\n","    activation: !name:torch.nn.LeakyReLU\n","    tdnn_blocks: 5\n","    tdnn_channels: [512, 512, 512, 512, 1500]\n","    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n","    tdnn_dilations: [1, 2, 3, 1, 1]\n","    lin_neurons: !ref <emb_dim>\n","\n","classifier: !new:custom_model.Classifier\n","    input_shape: [null, null, !ref <emb_dim>]\n","    activation: !name:torch.nn.LeakyReLU\n","    lin_blocks: 1\n","    lin_neurons: !ref <emb_dim>\n","    out_neurons: !ref <n_classes>\n","\n","# The first object passed to the Brain class is this \"Epoch Counter\"\n","# which is saved by the Checkpointer so that training can be resumed\n","# if it gets interrupted at any point.\n","epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n","    limit: !ref <number_of_epochs>\n","\n","# Objects in \"modules\" dict will have their parameters moved to the correct\n","# device, as well as having train()/eval() called on them by the Brain class.\n","modules:\n","    compute_features: !ref <compute_features>\n","    env_corrupt: !ref <env_corrupt>\n","    augmentation: !ref <augmentation>\n","    embedding_model: !ref <embedding_model>\n","    classifier: !ref <classifier>\n","    mean_var_norm: !ref <mean_var_norm>\n","```\n","The augmentation part is based on both `env_corrupt` (that adds noise and reverberation) and `augmentation`(that adds time/frequency dropouts and speed change).\n","For more information on these modules, please take a look at the tutorials on [enviromental corruption](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing) and the one on [speech augmentation](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing).\n","\n","\n","\n","We conclude the hyperparameter specification with the declaration of the optimizer, learning rate scheduler, and the checkpointer:\n","\n","\n","```yaml\n","# This optimizer will be constructed by the Brain class after all parameters\n","# are moved to the correct device. Then it will be added to the checkpointer.\n","opt_class: !name:torch.optim.Adam\n","    lr: !ref <lr_start>\n","\n","# This function manages learning rate annealing over the epochs.\n","# We here use the simple lr annealing method that linearly decreases\n","# the lr from the initial value to the final one.\n","lr_annealing: !new:speechbrain.nnet.schedulers.LinearScheduler\n","    initial_value: !ref <lr_start>\n","    final_value: !ref <lr_final>\n","    epoch_count: !ref <number_of_epochs>\n","\n","# This object is used for saving the state of training both so that it\n","# can be resumed if it gets interrupted, and also so that the best checkpoint\n","# can be later loaded for evaluation or inference.\n","checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n","    checkpoints_dir: !ref <save_folder>\n","    recoverables:\n","        embedding_model: !ref <embedding_model>\n","        classifier: !ref <classifier>\n","        normalizer: !ref <mean_var_norm>\n","        counter: !ref <epoch_counter>\n","```\n","\n","In this case, we use Adam as an optimizer and a linear learning rate decay over the 15 epochs.\n","\n","Let's now save the best model into a separate folder (useful for the inference part explained later):"]},{"cell_type":"code","metadata":{"id":"jwoN5Vq0dFYe","executionInfo":{"status":"ok","timestamp":1693465700001,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["# Create folder for best model\n","!mkdir /content/best_model/\n","\n","# Copy label encoder\n","!cp results/speaker_id/1986/save/label_encoder.txt /content/best_model/\n","\n","# Copy best model\n","!cp \"`ls -td results/speaker_id/1986/save/CKPT* | tail -1`\"/* /content/best_model/"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mnCM5xuy85P4"},"source":["### **Experiment file**\n","Let's now take a look into how the objects, functions, and hyperparameters declared in the yaml file are used in `train.py` to implement the classifier.\n","\n","Let's start from the main of the `train.py`:\n","\n","\n","```python\n","# Recipe begins!\n","if __name__ == \"__main__\":\n","\n","    # Reading command line arguments.\n","    hparams_file, run_opts, overrides = sb.parse_arguments(sys.argv[1:])\n","\n","    # Initialize ddp (useful only for multi-GPU DDP training).\n","    sb.utils.distributed.ddp_init_group(run_opts)\n","\n","    # Load hyperparameters file with command-line overrides.\n","    with open(hparams_file) as fin:\n","        hparams = load_hyperpyyaml(fin, overrides)\n","\n","    # Create experiment directory\n","    sb.create_experiment_directory(\n","        experiment_directory=hparams[\"output_folder\"],\n","        hyperparams_to_save=hparams_file,\n","        overrides=overrides,\n","    )\n","\n","    # Data preparation, to be run on only one process.\n","    sb.utils.distributed.run_on_main(\n","        prepare_mini_librispeech,\n","        kwargs={\n","            \"data_folder\": hparams[\"data_folder\"],\n","            \"save_json_train\": hparams[\"train_annotation\"],\n","            \"save_json_valid\": hparams[\"valid_annotation\"],\n","            \"save_json_test\": hparams[\"test_annotation\"],\n","            \"split_ratio\": [80, 10, 10],\n","        },\n","    )\n","```\n","\n","We here do some preliminary operations such as parsing the command line, initializing the distributed data-parallel (needed if multiple GPUs are used), creating the output folder, and reading the yaml file.\n","\n","After reading the yaml file with `load_hyperpyyaml`, all the objects declared in the hyperparameter files are initialized and available in a dictionary form (along with the other functions and parameters reported in the yaml file).\n","For instance,  we will have `hparams['embedding_model']`, `hparams['classifier']`, `hparams['batch_size']`, etc.\n","\n","We also run the data preparation script `prepare_mini_librispeech` that creates the data manifest files. It is wrapped with `sb.utils.distributed.run_on_main` because this operation writes the manifest files on disk and this must be done on a single process even in a multi-GPU DDP scenario. For more information on how to use multiple GPUs, [please take a look into this tutorial](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing).\n","\n","\n","#### **Data-IO Pipeline**\n","We then call a special function that creates the dataset objects for training, validation, and test.\n","\n","```python\n","    # Create dataset objects \"train\", \"valid\", and \"test\".\n","    datasets = dataio_prep(hparams)\n","```\n","\n","Let's take a closer look into that.\n","\n","\n","```python\n","def dataio_prep(hparams):\n","    \"\"\"This function prepares the datasets to be used in the brain class.\n","    It also defines the data processing pipeline through user-defined functions.\n","    We expect `prepare_mini_librispeech` to have been called before this,\n","    so that the `train.json`, `valid.json`,  and `valid.json` manifest files\n","    are available.\n","    Arguments\n","    ---------\n","    hparams : dict\n","        This dictionary is loaded from the `train.yaml` file, and it includes\n","        all the hyperparameters needed for dataset construction and loading.\n","    Returns\n","    -------\n","    datasets : dict\n","        Contains two keys, \"train\" and \"valid\" that correspond\n","        to the appropriate DynamicItemDataset object.\n","    \"\"\"\n","\n","    # Initialization of the label encoder. The label encoder assignes to each\n","    # of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n","    label_encoder = sb.dataio.encoder.CategoricalEncoder()\n","\n","    # Define audio pipeline\n","    @sb.utils.data_pipeline.takes(\"wav\")\n","    @sb.utils.data_pipeline.provides(\"sig\")\n","    def audio_pipeline(wav):\n","        \"\"\"Load the signal, and pass it and its length to the corruption class.\n","        This is done on the CPU in the `collate_fn`.\"\"\"\n","        sig = sb.dataio.dataio.read_audio(wav)\n","        return sig\n","\n","    # Define label pipeline:\n","    @sb.utils.data_pipeline.takes(\"spk_id\")\n","    @sb.utils.data_pipeline.provides(\"spk_id\", \"spk_id_encoded\")\n","    def label_pipeline(spk_id):\n","        yield spk_id\n","        spk_id_encoded = label_encoder.encode_label_torch(spk_id)\n","        yield spk_id_encoded\n","\n","    # Define datasets. We also connect the dataset with the data processing\n","    # functions defined above.\n","    datasets = {}\n","    hparams[\"dataloader_options\"][\"shuffle\"] = False\n","    for dataset in [\"train\", \"valid\", \"test\"]:\n","        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n","            json_path=hparams[f\"{dataset}_annotation\"],\n","            replacements={\"data_root\": hparams[\"data_folder\"]},\n","            dynamic_items=[audio_pipeline, label_pipeline],\n","            output_keys=[\"id\", \"sig\", \"spk_id_encoded\"],\n","        )\n","\n","    # Load or compute the label encoder (with multi-GPU DDP support)\n","    # Please, take a look into the lab_enc_file to see the label to index\n","    # mappinng.\n","    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n","    label_encoder.load_or_create(\n","        path=lab_enc_file,\n","        from_didatasets=[datasets[\"train\"]],\n","        output_key=\"spk_id\",\n","    )\n","\n","    return datasets\n","```\n","\n","The first part is just a declaration of the `CategoricalEncoder` that will be used to convert categorical labels into their corresponding indexes.\n","\n","\n","You can then notice that we expose the audio and label processing functions.\n","\n","The `audio_pipeline` takes the path of the audio signal (`wav`) and reads it. It returns a tensor containing the read speech sentence. The entry in input to this function (i.e, `wav`) must have the same name of the corresponding key in the data manifest file:\n","\n","```json\n","{\n","  \"163-122947-0045\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/163/122947/163-122947-0045.flac\",\n","    \"length\": 14.335,\n","    \"spk_id\": \"163\"\n","  },\n","}\n","```\n","\n","Similarly, we define another function called `label_pipeline` for processing the utterance-level labels and put them in a format usable by the defined model. The function reads the string `spk_id` defined in the JSON file and encodes it with the categorical encoder.\n","\n","We then create the `DynamicItemDataset` and connect it with the processing functions defined above. We define the **desired output keys** to expose. These keys will be available in the brain class within the batch variable as:\n","- batch.id\n","- batch.sig\n","- batch.spk_id_encoded\n","\n","The last part of the function is dedicated to the initialization of the label encoder. The label encoder takes in input the training dataset and assigns a different index to all the `spk_id` entries founded. These indexes will correspond to the output indexes of the classifier.\n","\n","[For more information on the data loader, please take a look into this tutorial](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n","\n","\n","\n","\n","After the definition of the datasets, the main function can go ahead with the  initialization and use of the brain class:\n","\n","```python\n","    # Initialize the Brain object to prepare for mask training.\n","    spk_id_brain = SpkIdBrain(\n","        modules=hparams[\"modules\"],\n","        opt_class=hparams[\"opt_class\"],\n","        hparams=hparams,\n","        run_opts=run_opts,\n","        checkpointer=hparams[\"checkpointer\"],\n","    )\n","\n","    # The `fit()` method iterates the training loop, calling the methods\n","    # necessary to update the parameters of the model. Since all objects\n","    # with changing state are managed by the Checkpointer, training can be\n","    # stopped at any point, and will be resumed on next call.\n","    spk_id_brain.fit(\n","        epoch_counter=spk_id_brain.hparams.epoch_counter,\n","        train_set=datasets[\"train\"],\n","        valid_set=datasets[\"valid\"],\n","        train_loader_kwargs=hparams[\"dataloader_options\"],\n","        valid_loader_kwargs=hparams[\"dataloader_options\"],\n","    )\n","\n","    # Load the best checkpoint for evaluation\n","    test_stats = spk_id_brain.evaluate(\n","        test_set=datasets[\"test\"],\n","        min_key=\"error\",\n","        test_loader_kwargs=hparams[\"dataloader_options\"],\n","    )\n","```\n","The `fit` method performs training, while the test is performed with the `evaluate` one. The training and validation data loaders are given in input to the fit method, while the test dataset is fed into the evaluate method.\n","\n","Let's now take a look into the most important methods defined in the brain class.\n","\n","\n","\n","#### **Forward Computations**\n","\n","Let's start with the `forward` function, which defines all the computations needed to transform the input audio into the output predictions.\n","\n","\n","```python\n","    def compute_forward(self, batch, stage):\n","        \"\"\"Runs all the computation of that transforms the input into the\n","        output probabilities over the N classes.\n","        Arguments\n","        ---------\n","        batch : PaddedBatch\n","            This batch object contains all the relevant tensors for computation.\n","        stage : sb.Stage\n","            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n","        Returns\n","        -------\n","        predictions : Tensor\n","            Tensor that contains the posterior probabilities over the N classes.\n","        \"\"\"\n","\n","        # We first move the batch to the appropriate device.\n","        batch = batch.to(self.device)\n","\n","        # Compute features, embeddings, and predictions\n","        feats, lens = self.prepare_features(batch.sig, stage)\n","        embeddings = self.modules.embedding_model(feats, lens)\n","        predictions = self.modules.classifier(embeddings)\n","\n","        return predictions\n","```\n","\n","In this case, the chain of computation is very simple. We just put the batch on the right device and compute the acoustic features. We then process the features with the TDNN encoder that outputs a fixed-size tensor. The latter feeds a classifier that outputs the posterior probabilities over the N classes (in this case the 28 speakers). Data augmentation is added in the prepare_features method:\n","\n","```python\n","    def prepare_features(self, wavs, stage):\n","        \"\"\"Prepare the features for computation, including augmentation.\n","        Arguments\n","        ---------\n","        wavs : tuple\n","            Input signals (tensor) and their relative lengths (tensor).\n","        stage : sb.Stage\n","            The current stage of training.\n","        \"\"\"\n","        wavs, lens = wavs\n","\n","        # Add augmentation if specified. In this version of augmentation, we\n","        # concatenate the original and the augment batches in a single bigger\n","        # batch. This is more memory-demanding, but helps to improve the\n","        # performance. Change it if you run OOM.\n","        if stage == sb.Stage.TRAIN:\n","            if hasattr(self.modules, \"env_corrupt\"):\n","                wavs_noise = self.modules.env_corrupt(wavs, lens)\n","                wavs = torch.cat([wavs, wavs_noise], dim=0)\n","                lens = torch.cat([lens, lens])\n","\n","            if hasattr(self.hparams, \"augmentation\"):\n","                wavs = self.hparams.augmentation(wavs, lens)\n","\n","        # Feature extraction and normalization\n","        feats = self.modules.compute_features(wavs)\n","        feats = self.modules.mean_var_norm(feats, lens)\n","\n","        return feats, lens\n","```\n","In particular, when the environmental corruption is declared in the yaml file, we concatenate in the same batch both the clean and the augmented version of the signals.\n","\n","This approach doubles the batch size (and this the needed GPU memory), but it implements a very **powerful regularizer**.  Having both the clean and the noisy version of the signal within the same batch forces the gradient to point into a direction of the parameter space that is **robust against signal distortions**.\n","\n","#### **Compute Objectives**\n","\n","Let's take a look now into the `compute_objectives` method that takes in input the targets, the predictions, and estimates a loss function:\n","\n","```python\n","    def compute_objectives(self, predictions, batch, stage):\n","        \"\"\"Computes the loss given the predicted and targeted outputs.\n","        Arguments\n","        ---------\n","        predictions : tensor\n","            The output tensor from `compute_forward`.\n","        batch : PaddedBatch\n","            This batch object contains all the relevant tensors for computation.\n","        stage : sb.Stage\n","            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n","        Returns\n","        -------\n","        loss : torch.Tensor\n","            A one-element tensor used for backpropagating the gradient.\n","        \"\"\"\n","\n","        _, lens = batch.sig\n","        spkid, _ = batch.spk_id_encoded\n","\n","        # Concatenate labels (due to data augmentation)\n","        if stage == sb.Stage.TRAIN and hasattr(self.modules, \"env_corrupt\"):\n","            spkid = torch.cat([spkid, spkid], dim=0)\n","            lens = torch.cat([lens, lens])\n","\n","        # Compute the cost function\n","        loss = sb.nnet.losses.nll_loss(predictions, spkid, lens)\n","\n","        # Append this batch of losses to the loss metric for easy\n","        self.loss_metric.append(\n","            batch.id, predictions, spkid, lens, reduction=\"batch\"\n","        )\n","\n","        # Compute classification error at test time\n","        if stage != sb.Stage.TRAIN:\n","            self.error_metrics.append(batch.id, predictions, spkid, lens)\n","\n","        return loss\n","```\n","The predictions in input are those computed in the forward method. The cost function is evaluated by comparing these predictions with the target labels. This is done with the Negative Log-Likelihood (NLL) loss.\n","\n","####**Other methods**\n","Beyond these two important functions, we have some other methods that are used by the brain class. The `on_state_starts` gets called at the beginning of each epoch and it is used to set up statistics trackers. The `on_stage_end` one is called at the end of each stage (e.g, at the end of each training epoch) and mainly takes care of statistic management, learning rate annealing, and checkpointing. [For a more detailed description of the brain class, please take a look into this tutorial](https://colab.research.google.com/drive/12bg3aUdr9mTfOGqcB5pSMABoIKPgiwcM?usp=sharing). For more information on checkpointing, [take a look here](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4LnRq1_cpPXZ"},"source":["## **Step 3: Inference**\n","\n","At this point, we can use the trained classifier to perform **predictions on new data**.  Speechbrain made available some classes ([take a look here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py)) such as the `EncoderClassifier` one that can make inference easier. The class can also be used to extract some embeddings at the output of the encoder.\n","\n","Let's see first how can we used it to load our best xvector model (trained on Voxceleb and stored on HuggingFace) to compute some embeddings and perform a speaker classification:\n"]},{"cell_type":"code","metadata":{"id":"uvvY0dCbx5Sv","colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"status":"error","timestamp":1693465703891,"user_tz":-330,"elapsed":3894,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}},"outputId":"6b20efa7-bf3c-4f2e-cc6f-e741b7c47bb0"},"source":["import torchaudio\n","from speechbrain.pretrained import EncoderClassifier\n","classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\")\n","signal, fs =torchaudio.load('/content/speechbrain/tests/samples/single-mic/example1.wav')\n","\n","# Compute speaker embeddings\n","embeddings = classifier.encode_batch(signal)\n","\n","# Perform classification\n","output_probs, score, index, text_lab = classifier.classify_batch(signal)\n","\n","# Posterior log probabilities\n","print(output_probs)\n","\n","# Score (i.e, max log posteriors)\n","print(score)\n","\n","# Index of the predicted speaker\n","print(index)\n","\n","# Text label of the predicted speaker\n","print(text_lab)\n"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-277a210277dd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspeechbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_hparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"speechbrain/spkrec-xvect-voxceleb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/speechbrain/tests/samples/single-mic/example1.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'speechbrain.pretrained'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"LAZci6oSzdh_"},"source":["For those of you interested in speaker verification, we also created an inference interface called `SpeakerRecognition`:"]},{"cell_type":"code","metadata":{"id":"l-enSWy_z8CF","executionInfo":{"status":"aborted","timestamp":1693465703892,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["from speechbrain.pretrained import SpeakerRecognition\n","verification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")\n","\n","file1 = '/content/speechbrain/tests/samples/single-mic/example1.wav'\n","file2 = '/content/speechbrain/tests/samples/single-mic/example2.flac'\n","\n","score, prediction = verification.verify_files(file1, file2)\n","\n","print(score)\n","print(prediction) # True = same speaker, False=Different speakers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbS7ZncE3IfM"},"source":["But, *how does this work with our custom classifier that we trained before*?\n","\n","At this point, some options are available to you. For a full overview of all of them, [please take a look into this tutorial](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing).\n","\n","We here only show how you can use the existing `EncoderClassifier` on the model that we just trained.\n","\n","\n","### **Use the EncoderClassifier interface on your model**\n","\n","The [EncoderClassidier class](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/pretrained/interfaces.py#L591) takes a pre-trained model and performs inference on it with the following methods:\n","\n","- **encode_batch**: applies the encoder to an input batch and returns some encoded embeddings.\n","- **classify_batch**: performs a full classification step and returns the output probabilities of the classifier, the best score, the index of the best class, and its label in text format (see example above).\n","\n","To use this interface with the model trained before, we have to create an **inference yaml** file which is a bit different from that use for training. The main differences are the following:\n","\n","1. You can remove all the hyperparameters and objects needed for training only. You can just keep the part related to the model definition.\n","2. You have to allocate a `Categorical encoder` object that allows you to transform indexes into text labels.\n","3. You have to use the pre-trainer to link your model with their corresponding files.\n","\n","The inference yaml file looks like that:\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ys41HanSaCys","executionInfo":{"status":"aborted","timestamp":1693465703893,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["%%writefile /content/best_model/hparams_inference.yaml\n","\n","# #################################\n","# Basic inference parameters for speaker-id. We have first a network that\n","# computes some embeddings. On the top of that, we employ a classifier.\n","#\n","# Author:\n","#  * Mirco Ravanelli 2021\n","# #################################\n","\n","# pretrain folders:\n","pretrained_path: /content/best_model/\n","\n","\n","# Model parameters\n","n_mels: 23\n","sample_rate: 16000\n","n_classes: 28 # In this case, we have 28 speakers\n","emb_dim: 512 # dimensionality of the embeddings\n","\n","# Feature extraction\n","compute_features: !new:speechbrain.lobes.features.Fbank\n","    n_mels: !ref <n_mels>\n","\n","# Mean and std normalization of the input features\n","mean_var_norm: !new:speechbrain.processing.features.InputNormalization\n","    norm_type: sentence\n","    std_norm: False\n","\n","# To design a custom model, either just edit the simple CustomModel\n","# class that's listed here, or replace this `!new` call with a line\n","# pointing to a different file you've defined.\n","embedding_model: !new:custom_model.Xvector\n","    in_channels: !ref <n_mels>\n","    activation: !name:torch.nn.LeakyReLU\n","    tdnn_blocks: 5\n","    tdnn_channels: [512, 512, 512, 512, 1500]\n","    tdnn_kernel_sizes: [5, 3, 3, 1, 1]\n","    tdnn_dilations: [1, 2, 3, 1, 1]\n","    lin_neurons: !ref <emb_dim>\n","\n","classifier: !new:custom_model.Classifier\n","    input_shape: [null, null, !ref <emb_dim>]\n","    activation: !name:torch.nn.LeakyReLU\n","    lin_blocks: 1\n","    lin_neurons: !ref <emb_dim>\n","    out_neurons: !ref <n_classes>\n","\n","label_encoder: !new:speechbrain.dataio.encoder.CategoricalEncoder\n","\n","# Objects in \"modules\" dict will have their parameters moved to the correct\n","# device, as well as having train()/eval() called on them by the Brain class.\n","modules:\n","    compute_features: !ref <compute_features>\n","    embedding_model: !ref <embedding_model>\n","    classifier: !ref <classifier>\n","    mean_var_norm: !ref <mean_var_norm>\n","\n","pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n","    loadables:\n","        embedding_model: !ref <embedding_model>\n","        classifier: !ref <classifier>\n","        label_encoder: !ref <label_encoder>\n","    paths:\n","        embedding_model: !ref <pretrained_path>/embedding_model.ckpt\n","        classifier: !ref <pretrained_path>/classifier.ckpt\n","        label_encoder: !ref <pretrained_path>/label_encoder.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDsp3atIiKSq"},"source":["As you can see, we only have the model definition here (not optimizers, checkpoiter, etc). The last part of the yaml file manages pretraining, where we bind model objects with their pre-training files created at training time.\n","\n","Let's now perform inference with the `EncoderClassifier` class:"]},{"cell_type":"code","metadata":{"id":"2fT8ON1iiuQY","executionInfo":{"status":"aborted","timestamp":1693465703893,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sherin Shibi","userId":"05792894419737785934"}}},"source":["from speechbrain.pretrained import EncoderClassifier\n","\n","classifier = EncoderClassifier.from_hparams(source=\"/content/best_model/\", hparams_file='hparams_inference.yaml', savedir=\"/content/best_model/\")\n","\n","# Perform classification\n","audio_file = 'data/LibriSpeech/train-clean-5/5789/70653/5789-70653-0036.flac'\n","signal, fs = torchaudio.load(audio_file) # test_speaker: 5789\n","output_probs, score, index, text_lab = classifier.classify_batch(signal)\n","print('Target: 5789, Predicted: ' + text_lab[0])\n","\n","# Another speaker\n","audio_file = 'data/LibriSpeech/train-clean-5/460/172359/460-172359-0012.flac'\n","signal, fs =torchaudio.load(audio_file) # test_speaker: 460\n","output_probs, score, index, text_lab = classifier.classify_batch(signal)\n","print('Target: 460, Predicted: ' + text_lab[0])\n","\n","# And if you want to extract embeddings...\n","embeddings = classifier.encode_batch(signal)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSxRDjEnB0FA"},"source":["The `EncoderClassifier` interface assumes that your model has the following modules specified in the yaml file:\n","\n","- *compute_features*: that manages feature extraction from the raw audio signal\n","- *mean_var_norm*: that performs feature normalization\n","- *embedding_model*: that converts features into fix-size embeddings.\n","- *classifier*: that performs a final classification over N classes on the top o the embeddings.\n","\n","If your model cannot be structured in this way, you can always customize the `EncoderClassifier` interface to fulfill your needs.\n","[Please, take a look into this tutorial for more information](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing).\n","    "]},{"cell_type":"markdown","metadata":{"id":"z3pu0M42Pqju"},"source":["## **Extension to different tasks**\n","In a general case, you might have your own data and classification task and you would like to use your own model. Let's comment a bit more on how you can customize your recipe.\n","\n","**Suggestion**:  start from a recipe that is working (like the one used for this template) and only do the minimal modifications needed to customize it. Test your model step by step. Make sure your model can overfit on a tiny dataset composed of few sentences. If it doesn't overfit there is likely a bug in your model."]},{"cell_type":"markdown","metadata":{"id":"tImuOg5XP3CY"},"source":["### **Train with your data on your task**\n","What about if I have to solve another utterance-level classification task such as **language-id**, **emotion recognition**, **sound classification**, **keyword spotting** on my data?\n","\n","All you have to do is:\n","1. Change the JSON with the annotations needed for your task.\n","2. Change the data pipeline in `train.py` to be compliant with the new annotations.\n","\n","#### **Change the JSON**\n","This tutorial expects  JSON files like this:\n","\n","```json\n","{\n","  \"163-122947-0045\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/163/122947/163-122947-0045.flac\",\n","    \"length\": 14.335,\n","    \"spk_id\": \"163\"\n","  },\n","  \"7312-92432-0025\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7312/92432/7312-92432-0025.flac\",\n","    \"length\": 12.01,\n","    \"spk_id\": \"7312\"\n","  },\n","  \"7859-102519-0036\": {\n","    \"wav\": \"{data_root}/LibriSpeech/train-clean-5/7859/102519/7859-102519-0036.flac\",\n","    \"length\": 11.965,\n","    \"spk_id\": \"7859\"\n","  },\n","}\n","```\n","\n","However, you can add here all the entries that you want. For instance, if you would like to solve a language-id task, the JSON file should look like this:\n","\n","```json\n","{\n","  \"sentence001\": {\n","    \"wav\": \"{data_root}/your_path/your_file1.wav\",\n","    \"length\": 10.335,\n","    \"lang_id\": \"Italian\"\n","  },\n","{\n","  \"sentence002\": {\n","    \"wav\": \"{data_root}/your_path/your_file2.wav\",\n","    \"length\": 12.335,\n","    \"lang_id\": \"French\"\n","  },\n","}\n","```\n","\n","If you would like to solve an emotion recognition task, it will look like that:\n","\n","\n","```json\n","{\n","  \"sentence001\": {\n","    \"wav\": \"{data_root}/your_path/your_file1.wav\",\n","    \"length\": 10.335,\n","    \"emotion\": \"Happy\"\n","  },\n","{\n","  \"sentence002\": {\n","    \"wav\": \"{data_root}/your_path/your_file2.wav\",\n","    \"length\": 12.335,\n","    \"emotion\": \"Sad\"\n","  },\n","}\n","```\n","To create the data manifest files, you have to **parse your dataset and create JSON files** with a unique ID for each sentence, the path of the audio signal (wav), the length of the speech sentence in seconds (length), and the annotations that you want.\n","\n","#### **Change train.py**\n","The only thing to remember is that the name entries in the JSON file must match with what the dataloader expectes in `train.py`.  For instance, if you defined an emotion key in JSON, you should have it in the dataio pipeline of `train.py` something like this:\n","\n","```python\n","    # Define label pipeline:\n","    @sb.utils.data_pipeline.takes(\"emotion\")\n","    @sb.utils.data_pipeline.provides(\"emotion\", \"emotion_encoded\")\n","    def label_pipeline(emotion):\n","        yield emotion\n","        emotion_encoded = label_encoder.encode_label_torch(emotion)\n","        yield emotion_encoded\n","```\n","\n","Basically, you have to replace the `spk_id` entry with the `emotion` one everywhere in the code. That's all!\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IVCCe6cXPzJ0"},"source":["### **Train with your own model**\n","At some point, you might have your own model and you would like to plug it into the speech recognition pipeline.\n","For instance, you might wanna replace our xvector encoder with something different.\n","\n","To do that, you have to create your own class and specify there the list of computations for your neural network. You can take a look into the models already existing in [speechbrain.lobes.models](https://github.com/speechbrain/speechbrain/tree/develop/speechbrain/lobes/models). If your model is a plain pipeline of computations, you can use the [sequential container](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/CRDNN.py#L14). If the model is a more complex chain of computations, you can create it as an instance of `torch.nn.Module` and define there the `__init__` and `forward` methods like [here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/Xvector.py#L18).\n","\n","Once you defined your model, you only have to declare it in the yaml file and use it in `train.py`\n","\n","**Important:**  \n","When plugging a new model, you have to tune again the most important hyperparameters of the system (e.g, learning rate, batch size, and the architectural parameters) to make it working well.\n","\n","\n","#### **ECAPA-TDNN model**\n","One model that we find particularly effective for speaker recognition is the ECAPA-TDNN one [implemented here](https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/lobes/models/ECAPA_TDNN.py).\n","\n","![picture](https://drive.google.com/uc?export=view&id=1iRwioXBSYKC2TnNg7ZFAXSydBQghxKBN)\n","\n","As shown in Figure, the ECAPA-TDNN architecture is based on the popular x-vector topology and it introduces **several enhancements** to create more robust speaker embeddings.\n","\n","The pooling layer uses a **channel- and context-dependent attention** mechanism, which allows the network to attend different frames per channel.\n","1-dimensional **SqueezeExcitation** (SE)  blocks rescale the channels of the intermediate frame-level feature maps to insert **global context** information in the locally operating convolutional blocks.\n","Next, the integration of 1-dimensional **Res2-blocks** improves performance while simultaneously reducing the total parameter count\n","by using grouped convolutions in a hierarchical way.\n","\n","Finally, **Multi-layer Feature Aggregation (MFA)** merges complementary information before the statistics pooling by concatenating the final frame-level feature map with an intermediate feature\n","maps of preceding layers.\n","\n","The network is trained by optimizing the **AAMsoftmax** loss on the speaker identities in the training corpus. The AAM-softmax is a powerful enhancement compared to the regular softmax loss in the context of fine-grained classification and verification problems. It directly optimizes the\n","cosine distance between the speaker embeddings.\n","\n","\n","The model turned out to work amazingly well for [speaker verification](https://arxiv.org/abs/2005.07143) and [speaker diarization](https://arxiv.org/abs/2104.01466). We found it very effective in other utterance-level classification tasks such as language-id, emotion recognition, and keyword spotting.\n","\n","[Please take a look into the original paper for more info](https://arxiv.org/abs/2005.07143)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W4pPJ0k3lJZj"},"source":["\n","\n","## **Conclusion**\n","\n","In this tutorial, we showed how to create an utterance-level classifier from scratch using SpeechBrain. The proposed system contains all the basic ingredients to develop a state-of-the-art system (i.e., data augmentation, feature extraction, encoding, statistical pooling, classifier, etc)\n","\n","We described all the steps using a small dataset only. In a real case you have to train with much more data (see for instance our [Voxceleb recipe](https://github.com/speechbrain/speechbrain/tree/develop/recipes/VoxCeleb))."]},{"cell_type":"markdown","metadata":{"id":"P-Trg_abjUTd"},"source":["## Related Tutorials\n","1. [YAML hyperpatameter specification](https://colab.research.google.com/drive/1Pg9by4b6-8QD2iC0U7Ic3Vxq4GEwEdDz?usp=sharing)\n","2. [Brain Class](https://colab.research.google.com/drive/1fdqTk4CTXNcrcSVFvaOKzRfLmj4fJfwa?usp=sharing)\n","3. [Checkpointing](https://colab.research.google.com/drive/1VH7U0oP3CZsUNtChJT2ewbV_q1QX8xre?usp=sharing)\n","4. [Data-io](https://colab.research.google.com/drive/1AiVJZhZKwEI4nFGANKXEe-ffZFfvXKwH?usp=sharing)\n","5. [ASR from Scratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing)\n","6. [Speech Features](https://colab.research.google.com/drive/1CI72Xyay80mmmagfLaIIeRoDgswWHT_g?usp=sharing)\n","7. [Speech Augmentation](https://colab.research.google.com/drive/1JJc4tBhHNXRSDM2xbQ3Z0jdDQUw4S5lr?usp=sharing)\n","8. [Environmental Corruption](https://colab.research.google.com/drive/1mAimqZndq0BwQj63VcDTr6_uCMC6i6Un?usp=sharing)\n","9. [MultiGPU Training](https://colab.research.google.com/drive/13pBUacPiotw1IvyffvGZ-HrtBr9T6l15?usp=sharing)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bfznz3wlEnn8"},"source":["# **About SpeechBrain**\n","- Website: https://speechbrain.github.io/\n","- Code: https://github.com/speechbrain/speechbrain/\n","- HuggingFace: https://huggingface.co/speechbrain/\n","\n","\n","# **Citing SpeechBrain**\n","Please, cite SpeechBrain if you use it for your research or business.\n","\n","```bibtex\n","@misc{speechbrain,\n","  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n","  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n","  year={2021},\n","  eprint={2106.04624},\n","  archivePrefix={arXiv},\n","  primaryClass={eess.AS},\n","  note={arXiv:2106.04624}\n","}\n","```"]}]}